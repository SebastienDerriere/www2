<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>
<head>
  <title>ADASS XVI Invited Speakers</title>
  <meta http-equiv="content-type"
 content="text/html; charset=ISO-8859-1">
</head>
<body>
<h1>ADASS XVI</h1>
<h2>A summary of suggestions for invited speakers, by theme</h2>
<br>
<br>
<h3>&nbsp;&nbsp; 3. Challenges and solutions for very large data</h3>
<br>
Glenn:<br>
I think the question of how to create robust, scalable and maintainable systems
fits with either topic 3 or 5 (which made the cut). I'd be curious if others
find this topic interesting -- or more to the point, do you find it a problem
at your institution. To restate the problem from below, I find that often
too much emphasis is placed on buying the cheapest hardware, and little thought
is given to operating costs, or sharing resources with similar projects.
Or maybe this problem is unique to STScI?<br>
At the Strasbourg meeting, Edwin Huizinga gave a talk on the redesigned HST
archive, which uses an EMC storage array talking to a Sunfire compute server,
and that talk contained elements of this thread. I don't have a specific
speaker in mind (yet)...<br>
<br>
<big><b>Jeff Kantor</b></big>, LSST Project Manager<br>
(Athol)<br>
 <b>Data management solutions for the very large data rates expected for
LSST</b><br>
I realize we invited Tim Axelrod last year, so the POC will need to weight
this. LSST data rates are of special interest though.<br>
<br>
Dick:<br>
I agree this is very interesting, and worth revisiting after 3 years. At
NOAO we have moved to an Apple-based mass data storage solution, from an
initial (but short-lived) experiment with a rack of independent JetStore
boxes. The point for us was scaleable volume management, though of course
lots of other freebees came with the solution (integrated system health monitoring,
failure recovery, etc.).<br>
You might consider getting a couple of speakers... one from a major astronomy
center, and one from industry. Our Apple rep was pretty good, and kept the
sales/marketing stuff to a minimum. I think if we were to emphasize the academic/pragmatic
nature of the topic &amp; audience, a industry person would respond appropriately.<br>
<br>
<big><b>John Good</b></big>, IRSA<br>
(Tom)<br>
IRSA's development of a highly scaleable and extensible hardware and architecture
for curating and serving these data at reasonable cost, while allowing users
rapid (and in most cases) real time discovery and access to the data<br>
IRSA now serves over 100 catalogs, and over 50 image and spectral datasets,
approaching 20 TB in size.&nbsp;&nbsp; These data are eclectic, in that they
include all-sky surveys and targeted datasets, data from the X-rays to the
submm, and data sets that range in size from a few MB to 10 MB. IRSA has
developed a highly scaleable and extensible hardware and architecture for
curating and serving these data at reasonable cost, while allowing users
rapid (and in most cases) real time discovery and access to the data.<br>
<br>
<big><b>Tom Handley</b></big>, IPAC<br>
(Tom)<br>
<b>Spitzer, an example "enterprise architecture"</b><br>
Spitzer's flexible, scaleable hardware and software architecture has been
key in its ability to handle explosion of data products and evolving data
definitions and changing data quality requirements.&nbsp; Spitzer is generating
(depending on the campaign and instrument) about 10TB of pre-archive data
every 14 to 20 days.&nbsp; This generally reduces to between 3TB-6TB of standard
products, again depending on the campaign and instrument.&nbsp; The files
and data are growing in leaps and bounds.<br>
This talk will address Spitzer's responses to on-going data, quality, and
processing requirements via an evolving architecture, evolving implementation,
evolving technology - including how robust or not was the original architecture
to allow Spitzer to accomodate this exploding data and processing needs.<br>
<br>
<big><b>Bongki Moon</b></big>, U of AZ (CS)<br>
(Dick)<br>
<b>High performance database systems and scalable web servers</b><br>
I would like to suggest another speaker for the theme "Challenges and 
solutions for very large data." You may recall that we invited Dr. Bongki Moon 
(U of Arizona CS department) last year under the theme of data mining. In the 
end he declined our offer to speak, mostly I think because he had moved on to 
his current area of work: high performance database systems and scalable web 
servers. He is of late working with LSST on HP-DBMS. For more details, see 
http://www.cs.arizona.edu/~bkmoon/. As he is local to Tucson, it should be 
possible to interest him in speaking.<br>
<br>
<h3>&nbsp;&nbsp; 1. Advances in imaging and calibration algorithms</h3>
<br>
<big><b>Tim Cornwell</b></big>, ATNF<br>
(Athol)<br>
<b>Calibration and imaging challenges for the next-generation radio astronomy
instrument / Square Kilometer Array</b><br>
Calibration and imaging challenges for the next-generation radio astronomy
instrument.<br>
<br>
<h3>&nbsp; 11. Quality management in astronomical data management systems</h3>
<br>
<b><big>Nicole Radziwill</big></b>, NRAO/Greenbank<br>
(Dick)<br>
Her professional pedigree in QM is remarkable (see http://www.gb.nrao.edu/~nradziwi/).
She gave an excellent, if abbreviated, contributed talk at ADASS in Spain.
I was so impressed that I invited her out to NOAO/Tucson to give a seminar
and spend some time with our developers and our management team. She is a
very good, if not riveting speaker, and she knows her material in great depth.<br>
Her role at NRAO is as manager of a team of developers who write control
software, etc. If you like, I can dig up the abstract of the seminar she
gave only last week.<br>
<br>
<b><big>Ian Evans</big></b>, CfA/CXC<br>
(Arnold)<br>
<b>Verification and Validation: Data Quality control in Chandra automated
data processing</b><br>
Before the processed Chandra data are delivered to the users, they first
need to pass through a Verification and Validation step. Where this originally
was a completely manual operation, it has now largely been automated and
only requires a few minutes of human inspection.&nbsp; Ian Evans has managed
the development of the V&amp;V pipeline.<br>
<br>
<b><big>Mike Watson</big></b>, SSC/University of Leicester<br>
(Carlos)<br>
<b>Reprocessing of all XMM-Newton data and automatic X-ray catalogue extraction</b>
<br>
The SSC is conducting at the moment the reprocessing of the whole data from
the XMM-Newton mission. This will lead to the re-population of the XMM-Newton
archive with a set of high-level scientific products, highly homogenoeous
in terms of quality and calibrations, as well as to the generation of the
largest catalogue of hard X-ray sources ever compiled (~ 250 - 300 K entries,
mostly previously unknown sources).<br>
<br>
<b><big>Giuseppina Fabbiano</big></b>, CfA/CXC<br>
(Arnold)<br>
<b>The Chandra Source Catalog Project</b><br>
The software development for the Chandra Source Catalog has been ongoing
for several years, but is converging now on a production start in the fall.
&nbsp;The intent is to include all point sources and moderately extended
sources that are observed in all Chandra imaging observations, using the
entire field-of-view. &nbsp;Ultimately, the catalog will hold a rich variety
of parameters for each source, as well as data objects (event files, images,
spectra, light curves) that will allow users to submit sophisticated queries
based on custom analysis of those data objects. &nbsp;The size of the catalog
is expected to exceed 400,000 sources and its astrometric quality unmatched
for the foreseeable future. Of particular interest are the limits: completeness,
false detection rate, and upper limits as a function of position. &nbsp;At
the same time, defining a proper characterization of the catalog that will
allow the derivation of such limits dynamically is one of the most challenging
quality control aspects of the project, as sensitivity is highly variable.<br>
Pepi Fabbiano heads the Data Systems division of the Chandra X-ray Center.<br>
<br>
<h3>&nbsp;&nbsp; 2. Modern grid computing in astronomy</h3>
<br>
<b><big>Matthew Graham</big></b>, CACR<br>
(Athol)<br>
<b>How web services are used in the VO; current issues relating to compatibility
and practical deployment of web-services in astronomy.</b><br>
How web services are used in the VO; current issues relating to compatibility
and practical deployment of web-services in astronomy.<br>
<br>
<b><big>Von Welch</big></b>, NCSA<br>
(Athol)<br>
<b>Modern grid security mechanisms; their practical use in large projects
with a focus on astronomy.</b><br>
Modern grid security mechanisms; their practical use in large projects with
a focus on astronomy.<br>
<br>
<b><big>Joe Mohr</big></b>, UIUC Astronomy Professor, Dark Energy Survey
project (DES)<br>
(Athol)<br>
<b>Current and future grid issues from a science user perspective.</b><br>
Joe plays an important role in the Dark Energy Survey project (DES),&nbsp;
a ground-based NOAO-NSF-DOE project to characterize dark energy. They have
been actively using the gid to develop their data management, calibration,
and imaging software, and Joe would be able to address current and future
grid issues from a science user perspective.<br>
<br>
<b><big>Bruce Berriman</big></b>, IPAC/IRSA<br>
(Tom)<br>
<b>Montage</b><br>
IPAC/IRSA, as a part of VO, has constructed an on-request Grid service for
computing image mosaics with the Montage software.&nbsp; Users order mosaics
from a web portal at IPAC, and the service is run&nbsp; on the Teragrid.&nbsp;
It is currently under evaluation by scientists, and we are aiming for public
deployment by early Summer.&nbsp; The architecture includes the web portal,
mechanisms for accepting and monitoring processes, building workflows and
executing them, and staging results for users to pick up.&nbsp; The architectural
components are all loosely coupled, and consist of established technologies
such as Condor plugged in to Pegasus and plugged into our our newly minted
request management system.&nbsp; To my knowledge, this is the first such
service in astronomy.<br>
<br>
<h3>&nbsp;&nbsp; 5. Architectures for large astronomy software systems</h3>
<br>
<b><big>Joe Mazzarella</big></b>, IPAC<br>
(Tom)<br>
<b>NED, 15 years and growing</b><br>
NED's flexible architecture is built around a master list of extragalactic
objects for which cross-identifications of names have been established, accurate
positions and redshifts entered to the extent possible, and some basic data
collected.&nbsp; Bibliographic references relevant to individual objects
have been compiled, and abstracts of extragalactic interest are kept on line.&nbsp;
Detailed and referenced photometry, position, and redshift data, have been
taken from large compilations and from the literature.&nbsp; NED also includes
images from 2MASS, from the literature, and from the Digitized Sky Survey.&nbsp;
NED's data and references are being continually updated, with revised versions
being put on-line every 2-3 months.&nbsp; This talk would describe the evolution
of NED during its 15 years -- in cross-correlating, assimilating, managing
and serving massive datasets for all of extragalactic astronomy. NED is especially
well know for it quality management as well as it all-inclusive holdings
- data quality, an important attribute will be detailed.<br>
<br>
<b><big>Fran&#231;oise Genova</big></b>, CDS<br>
(Fran&#231;ois)<br>
<b>The many faces of Simbad</b><br>
The recent evolutions, with the emergence of the very large surveys,
the ubiquitous access to numerous data servers, have a deep impact
on Simbad's role, its contents, its connectivity with the other data
bases and data centers, and the procedures now in use for its daily data
checking and updates.  The talk will present the changes in Simbad's roles,
the methods developed to improve the synergy between Simbad and the external
data collections, as well as the impact on the evolution of the Simbad 
architecture.<br>
<br>
<b><big>Kenny Glotfelty</big></b>, CfA/CXC<br>
(Arnold)<br>
<b>Building processing pipelines for the Chandra automated processing system</b><br>
Chandra data undergo a fair amount of processing before being delivered to
the users - that is in the nature of X-ray data.&nbsp; The Automated Processing
system consists of about two dozen pipelines that are built up of atomic
component tools.&nbsp; Kenny is the software engineer in charge of the pipeline
development team.<br>
<br>
<b><big>Wil O'Mullane</big></b>, ESAC<br>
(Carlos)<br>
<b>GAIA</b><br>
Gaia will conduct a galactic census, measuring to within twenty microarcsecond
about one thousand million objects over five years. Each object will be observed
on average eighty times yielding a kinematic map of the galaxy with profound
impact in many branches of astronomy. The Data processing required to produce
the Gaia catalogue is immense and will be distributed over at least five
European countries. The organisational and computational challenge is daunting.<br>
<br>
<b><big>Dr. Yuji Shirasaki</big></b>, JVO, NAOJ<br>
(Koh-Ichiro)<br>
<b>The SUBARU Advanced Data Service</b><br>
He has been working for the Japanese Virtual Observatory (JVO) project
at National Astronomical Observatory Japan (NAOJ). He is now the
co-chair of VOQL (VO Query Language) working group of the IVOA and
leading the activity for defining the international standard.<br>

He is also developing "the SUBARU Advanced Data Service", which
is the first VO compliant SKYNODE service to provide the reduced
observational data. Grid computing environment is also constructed
to process the large amount of dataset (~10TB) obtained by the SUBARU
telescope.<br>

He is now conducting two kinds of sience use cases on this VO service.
One is "cosmic string search through the gravitational lensing" and the
other is "study of the cosmic large scale structure from the QSO
environment".<br>

He will describe the architecture of the skynode data service and the
grid computing system, and also introduce the experience of scientific
study on such system.
<br>
<br>
<b><big>Kjeld van der Schaaf</big></b>, Astron<br>
(Tim)<br>
<b>Architecture for LOFAR</b><br>
LOFAR is a fascinating multi-discipline project, rooted in low  
frequency radio astronomy but also with application to environmental  
monitoring in the Netherlands. The data volumes and processing load  
are both very large and the challenges in processing algorithms  
immense. We've heard talks about the calibration strategy from  
Noordman and on HPC from van der Schaaf but an architecture talk  
would be worthwhile and would fit into the program nicely.
<br>
<br>
<h3>&nbsp; 10. Solar neighborhood/ planetary astronomy</h3>
<br>
<b><big>Jeremy Kubica</big></b><br>
(Dick)<br>
<b>Variable tree algorithms and the efficient discovery of spatial associations
and structure</b><br>
I suggest we invite Dr. Jeremy Kubica, who is a recent graduate of the Ph.D.
program at Carnegie Mellon University's Robotics Institute, where he was
supported by a fellowship from the Hertz Foundation. His current research
focuses on large-scale data mining problems and the search for structure
in large, noisy data sets. Previously he received a B.S. in Computer Science
from Cornell University and a M.S. in Robotics from Carnegie Mellon University.
You can find an slightly outdated, but useful web page at: http://www.autonlab.org/autonweb/people/gradstudents/10229.html.<br>
Dr. Kubica was recently invited to give a colloquium at the IfA at U.Hawaii,
at the invitation of the PanSTARRS project. The title and abstract of that
talk is appended below.<br>
<br>
Title of Kubica's talk:<br>
VARIABLE TREE ALGORITHMS AND THE EFFICIENT DISCOVERY OF SPATIAL ASSOCIATIONS
AND STRUCTURE<br>
Abstract:<br>
Finding sets of points that conform to an underlying spatial model is a conceptual
simple, but potentially expensive, task. In this talk I consider the computational
issues inherent in extracting structure from large, noisy data sets. I discuss
how a trade-off exists between traditional search algorithms based on spatial
data structures and more recent multiple tree algorithms, leaving both approaches
with potential drawbacks.<br>
I describe a new type of tree-based search algorithm that uses a dynamic,
variable number of tree nodes to adapt to both structure in the data and
search state itself. I show that this new approach addresses potential drawbacks
in both the constructive and multiple tree approaches. Further, I show that
this algorithm leads to significant benefits on a variety of problem domains.<br>
While the problem of finding spatial structure arises in a wide range of
domains, the primary motivating problem throughout this talk is the task
of efficiently linking faint asteroid detections. Here the goal is to link
together individual point observations in order to find and extract new asteroid
trajectories in the data. Future astronomical surveys will provide a wealth
of observational data to this end, greatly increasing both the ability to
find new objects and the scale of the problem. Thus efficient algorithms
are vital to effectively handle the massive amount of data that is expected.<br>
<br>
<b><big>Christophe Arviset</big></b>, ESAC<br>
(Carlos)<br>
<b>ESA's Planetary Science Archive</b><br>
ESA's Planetary Science Archive (PSA, http://www.rssd.esa.int/PSA) contains
scientific data from all ESA's planetary missions (currently Mars Express,
Rosetta, Smart-1, Giotto, later on Huygens, Venus Express). The PSA is based
on the same architecture and design as ESA's astronomy archives (ISO, XMM-Newton,
Integral) and has been developed by the same team. Re-use of human expertise
as well as software <br>
technologies (Java, XML, RDBMS) and even code has allowed fast and reliable
development of the PSA, as well as making the maintenance easier, while also
allowing an innovative approach for searching based on a graphical map, complementing
the standard search based on filling in forms with query criteria. Additionnally,
a specific dataset validator software has been developed to ensure smooth
ingestion of datasets into the PSA, as they are being produced by dozens
of instrument teams over Europe.<br>
<br>
<br>
<br>
<h3>The themes that didn't make it:</h3>
<br>
&nbsp;&nbsp; 9. Solar Astronomy data analysis<br>
&nbsp;&nbsp; 7. Advances and challenges in real-time computing and control<br>
&nbsp;&nbsp; 4. Software engineering practice and experience in astronomy<br>
&nbsp;&nbsp; 8. Development tools and environments<br>
&nbsp;&nbsp; 6. The role of computing in education and outreach<br>
<br>
</body>
</html>
